# Общее

Канал: @iad_2024
wiki: http://wiki.cs.hse.ru/Основы_глубинного_обучения
# Семинар 1 - Intro to PyTorch - (19.09.24)

Torch

```python
x = torch.rand(5, 3)
x.shape == x.size()

x @ x.transpose(1, 0) == torch.matmul(x, x.transpose(1, 0))
x.mm(x.t())

x.view([3, 5])  # reshape

torch.empty([5, 3])
torch.zeroes([5, 3])
torch.ones([5, 3])
torch.tensor([5, 3])
torch.randn_like(x)
```

```python
x.unsqueeze(0)
x.unsqueeze_(0)  # inplace
x.squeeze(0)
```

```python
require_grad

x.grad
x.grad.data.zero_()
```

```python
class RDataset(torch.util.data.Dataset):
	def __len__(self):
		...

	def __getitem__(self, idx):
		return {
			"sample": ...
			"target": ...
		}

dataset = RDataset(...)
dataloader = torch.util.data.DataLoader(dataset, batch_size=32, num_workers=4)
batch = next(iter(dataloader))
```

```python
class Model(torch.nn.Model):
	...
```

MNIST

# Лекция 1 - Intro to Deep Learning

Классический CV:
- Считаем признаки
- Обучаем на них град. бустинг

Современный CV:


Классическое NLP:
- Марковские цепи

Современные NLP:
- GPT

## Граф вычислений (нейронная сеть)

$x^{(0)}$ - признаки объекта
$h_i$ - слой
$x^{(1)}$ - выход

Полносвязные слои:
- На входе n чисел, на выходе m
- Каждый выход - линейная модель над входом

Нелинейность:
- Два полносвязных слоя подряд ~ одному полносвязному слою $\rightarrow$ нужно добавлять нелинейную функцию после полносвязного слоя

- Сигмоида
- ReLU

_Теорема Цыбенко_ - пусть $g(x)$ - непрерывная функция, тогда можно построить двуслойную нейронную сеть, приближающую $g(x)$ с любой заранее заданной точностью
