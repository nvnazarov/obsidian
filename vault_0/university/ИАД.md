# Общее

Канал: @iad_2024
wiki: http://wiki.cs.hse.ru/Основы_глубинного_обучения
# (19.09.24) С1 - Введение в PyTorch

Torch

```python
x = torch.rand(5, 3)
x.shape == x.size()

x @ x.transpose(1, 0) == torch.matmul(x, x.transpose(1, 0))
x.mm(x.t())

x.view([3, 5])  # reshape

torch.empty([5, 3])
torch.zeroes([5, 3])
torch.ones([5, 3])
torch.tensor([5, 3])
torch.randn_like(x)
```

```python
x.unsqueeze(0)
x.unsqueeze_(0)  # inplace
x.squeeze(0)
```

```python
require_grad

x.grad
x.grad.data.zero_()
```

```python
class RDataset(torch.util.data.Dataset):
	def __len__(self):
		...

	def __getitem__(self, idx):
		return {
			"sample": ...
			"target": ...
		}

dataset = RDataset(...)
dataloader = torch.util.data.DataLoader(dataset, batch_size=32, num_workers=4)
batch = next(iter(dataloader))
```

```python
class Model(torch.nn.Model):
	...
```

MNIST

Классификация: на выход несколько чисел.

`SGD(model.parameters(), lr=0.05)` - можно передавать не все параметры (обучение будет быстрее, но не будет затрагивать все параметры).

`[x for x in model.named_parameters()]` - посмотреть все параметры.

Эпоха - проход по всем батчам.
# (19.09.24) Л1 - Введение в Deep Learning

Классический CV:
- Считаем признаки
- Обучаем на них град. бустинг

Современный CV:


Классическое NLP:
- Марковские цепи

Современные NLP:
- GPT
## Граф вычислений (нейронная сеть)

$x^{(0)}$ - признаки объекта
$h_i$ - слой
$x^{(1)}$ - выход

Полносвязные слои:
- На входе n чисел, на выходе m;
- Каждый выход - линейная модель над входом.

Нелинейность:
- Два полносвязных слоя подряд ~ одному полносвязному слою $\rightarrow$ нужно добавлять нелинейную функцию после полносвязного слоя.

- Сигмоида
- ReLU

_Теорема Цыбенко_ - пусть $g(x)$ - непрерывная функция, тогда можно построить двуслойную нейронную сеть, приближающую $g(x)$ с любой заранее заданной точностью.
# (26.09.24) С2 - Свертка изображения

Свертка - `Conv2d`:
- `kernel_size` - размер фильтра;
- `stride` - шаг (на сколько пикселей перемещается фильтр);
- `padding` - расширение картинки.

Свертки детектируют паттерны на картинках. Свертки можно обучать, а можно использовать готовые, настроенные на определенные паттерны.

В `torch` размерность батча из картинок должна быть (1, 3, w, h), хотя обычно каналы указываются последними (то есть обычно (w, h, 3)). Перевести в формат `torch`:

```python
img_tensor.permute(2, 0, 1).unsqueeze(0)
```

Попробуем применить оператор Собеля (для детектирования горизонтальных границ):

```python
sobel = [
	[-1,-2,-1],
	[ 0, 0, 0],
	[ 1, 2, 1],
]

kernel = [[sobel, sobel, sobel]]  # по одному фильтру на каждый канал
kernel = torch.tensor(kernel, dtype=torch.float)

img_conv = conv2d(img_tensor, kernel)
img_conv = img_conv.permute(0, 2, 3, 1)  # возвращаем к размерности (1, 3, w, h)

# рисуем ...
```

Пример с MNIST:
- числа для нормализации (`mean` и `std`) можно брать из датасета ImageNet.

```python
transform = torchvision.transforms.Compose(
	[
		torchvision.transforms.ToTensor(),  # make tensor from numpy array
		torchvision.transforms.Normalize((0.1307,), (0.3081,)),
	]
)

# цикл обучения
def train(model, optimizer):
	n_epochs = 5
	for epoch in range(n_epochs):
		...

# полносвязная NN
model = nn.Sequential(
	nn.Flatten(),
	nn.Linear(28*28, 128),
	nn.ReLU(),
	nn.Linear(128, 10),
)

optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
train(model, optimizer)

# NN со сверточными слоями
model = nn.Sequential(
	nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5),
	nn.ReLU(),
	nn.MaxPool2d(kernel_size=4),  # уменьшить картинку в 4 раза по каждой стороне
	nn.Flatten(),
	nn.Linear(6*6*10, 128),
	nn.ReLU(),
	nn.Linear(128, 10),
)

optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
train(model, optimizer)
```
# (26.09.24) Л2 - Backprop & Сверточные сети
## Backpropagation

Метод для подсчета ошибки (производных).

В Backprop каждая частная производная вычисляется один раз. Вычисление производных по слою `N` сводится к перемножению матрицы производных по слою `N+1` и некоторых векторов.
## FC для изображений

MNIST:
- 60.000 изображений 28x28;
- Центрированы, фон убран.

Input Layer (784)
Hidden Layer (FC на 1000 нейронов)
Output Layer (10)

Если взять картинку из обучающей выборки и немного подвинуть, то результат может сильно ухудшиться.

FC для изображений:
- Слишком много параметров;
- Легко могут переобучиться;
- Не учитывают специфику изображений (сдвиги, небольшие изменения формы, ...);
- Один из лучших способов борьбы с переобучением - уменьшение числа параметров.
## Сверточные слои

Свертка:

```
1010    10 * 12   =   5__
0110    01   34       ___
1010      (filter)    ___
1011
```

Максимум свертки инвариантен к сдвигу.

Примеры сверток в CV: Horizontal Sobel kernel, ...

$$ Im_{out}(x, y) = \sum_{i=-d}^d \sum_{j=-d}^d (K(i,j) \cdot Im_{in}(x+i, y+j) + b) $$
- Local Connectivity (цвет пикселя зависит только то небольшого участка исходного изображния);
- Shared Weight (веса одни и те же для всех пикселей результирующего изображения).
# (03.10.24) С3 - Классификация изображений

Датасет CIFAR.


# (03.10.24) Л3 - Сверточные сети

Обычно в изображении несколько каналов:
$$ Im_{out}(x, y) = \sum_{i=-d}^d \sum_{j=-d}^d \sum_{c=1}^C (K(i,j,c) \cdot Im_{in}(x+i,y+j,c) + b) $$
Одна свертка выделяет конкретный паттерн.
Нам интересно искать много паттернов.
Сделаем результат трехмерным:
$$ Im_{out}(x,y,t) = \sum_{i=-d}^d \sum_{j=-d}^d \sum_{c=1}^C (K_t(i,j,c) \cdot Im_{in}(x+i,y+j,c) + b_t) $$
Всего параметров: $((2d + 1)^2*C+1)*T$.
## Receptive field

Поле восприятия - от какого участка исходного изображения зависит значение выходного пикселя.
## Strides

Пропуски - фильтр применяется не ко всем пикселям, а перепрыгивает промежуточные.

```
|%.%.%.%|
|.......|
|%.%.%.%|
|.......|
```

Поле восприятия растет быстрее, если есть пропуски.
## Dilated convolution

Раздутые свертки - фильтр умножается на на цельный кусок входного изображения (например, 3x3), а на кусок с пропусками.
## Pooling

Еще один вид слоя.
### Max-pooling, Average-pooling, ...

## Padding

Если просто применять фильтр, то картинка будет уменьшаться в размере.

- Zero padding - добавляем нули на границе;
- Reflection padding;
- Replication padding.
## Типичная архитектура

`Conv` -> `Max Pooling` -> (repeat) -> (flattened) `FC` -> `FC`

И нелинейность после каждого слоя.
### AlexNet
### Визуализация фильтров
# (10.10.24) Л4 - Оптимизация в глубинном обучении

Full GD, Stochastic GD, Mini-batch GD

Batch size в Mini-batch GD:
- Лучше начинать с маленького batch'а, а потом увеличивать его;
## Модификации GD
### Momentum

$h_t = \alpha h_{t-1} + \eta_t \grad Q (w^{t-1})$ - накопленный градиент
$w^t = w^{t-1} - h_t$

$h_t = \alpha h_{t-1} + \eta_t \grad Q (w^{t-1} - \alpha h_{t-1})$ - Nesterov Momentum
### AdaGrad
### RMSProp
## Борьба с переобучением
### Dropout

- Можно определить как слой `d(x)`;
- Параметров нет, единственный гиперпараметр - вероятность удаления нейрона `p`;
- `d(x) = 1/(1-p)*m*x`, где `m` - вектор из распеределения Бернулли.
## Нормализации

Covariate Shift

Batch Normalization
# Л5